{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "import matplotlib.pyplot as plt\n",
    "import cutslib as cl\n",
    "import cutslib.glitch as gl\n",
    "from cutslib.visual import array_plots\n",
    "import matplotlib.cm as cm\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "def setup_logging(log_file='active_learning.log'):\n",
    "    \"\"\"Configure logging.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s:%(levelname)s:%(message)s'\n",
    "    )\n",
    "\n",
    "def load_data(df_dir, df_test_name, df_train_name, df_train_high_prob_name):\n",
    "    \"\"\"Load DataFrames from CSV files.\"\"\"\n",
    "    try:\n",
    "        df_test = pd.read_csv(os.path.join(df_dir, f'{df_test_name}.csv'))\n",
    "        df_train = pd.read_csv(f'/home/simran/examples_for_zoey/plots_for_labelling/manifest.csv')\n",
    "        df_train_high_prob = pd.read_csv(f'/home/simran/examples_for_zoey/plots_for_labelling/manifest.csv')\n",
    "        logging.info(\"Successfully loaded all DataFrames.\")\n",
    "        return df_test, df_train, df_train_high_prob\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def initialize_active_learner(df_train, cols_t, n_trees=50, max_depth=15):\n",
    "    \"\"\"Initialize the Active Learner with RandomForestClassifier.\"\"\"\n",
    "    try:\n",
    "        X_start = df_train[cols_t].to_numpy()\n",
    "        Y_start = df_train['Train_Lab'].to_numpy()\n",
    "        \n",
    "        learner = ActiveLearner(\n",
    "            estimator=RandomForestClassifier(\n",
    "                criterion='entropy',\n",
    "                n_estimators=n_trees,\n",
    "                random_state=1,\n",
    "                n_jobs=2,\n",
    "                max_depth=max_depth\n",
    "            ),\n",
    "            query_strategy=uncertainty_sampling,\n",
    "            X_training=X_start,\n",
    "            y_training=Y_start\n",
    "        )\n",
    "        logging.info(\"Initialized ActiveLearner.\")\n",
    "        return learner, X_start, Y_start\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing ActiveLearner: {e}\")\n",
    "        raise\n",
    "\n",
    "def perform_active_learning(learner, df_test, cols, cols_t, n_queries=5):\n",
    "    \"\"\"Perform the active learning loop.\"\"\"\n",
    "    try:\n",
    "        X_add = df_test[cols].to_numpy()\n",
    "        Y_add = df_test['Train_Lab'].to_numpy()\n",
    "        df_new = pd.DataFrame()\n",
    "        \n",
    "        for _ in range(n_queries):\n",
    "            query_idx, query_instance = learner.query(X_add[:, :len(cols_t)])\n",
    "            \n",
    "            if isinstance(query_idx, (int, np.integer)):\n",
    "                query_idx = [query_idx]\n",
    "            \n",
    "            # Extract the queried instance(s)\n",
    "            queried_X = X_add[query_idx, :len(cols_t)]\n",
    "            queried_y = Y_add[query_idx]\n",
    "            \n",
    "            # Append to df_new with all necessary columns from df_test\n",
    "            df_new = pd.concat([df_new, df_test.iloc[query_idx][cols].reset_index(drop=True)], ignore_index=True)\n",
    "            logging.info(f\"Appended {len(query_idx)} data point(s) to df_new.\")\n",
    "            \n",
    "            # Remove the queried instance(s) from X_add and Y_add\n",
    "            X_add = np.delete(X_add, query_idx, axis=0)\n",
    "            Y_add = np.delete(Y_add, query_idx, axis=0)\n",
    "            logging.info(f\"Removed {len(query_idx)} data point(s) from the unlabeled pool.\")\n",
    "            \n",
    "            # Teach the learner with the new instance(s)\n",
    "            learner.teach(\n",
    "                X=queried_X,\n",
    "                y=queried_y\n",
    "            )\n",
    "            logging.info(f\"Taught the learner with {len(query_idx)} new data point(s).\")\n",
    "        \n",
    "        return df_new, X_add, Y_add\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during active learning: {e}\")\n",
    "        raise\n",
    "\n",
    "def select_high_probability_data(df_train_high_prob, df_train, classifier, cols_t, cols, threshold=0.6, num_per_class=5):\n",
    "    \"\"\"Select high-probability data points based on classifier predictions.\"\"\"\n",
    "    try:\n",
    "        # Merge to ensure all required columns are present\n",
    "        merged_df = df_train_high_prob.merge(\n",
    "            df_train,\n",
    "            on='Start Index',  # Replace with the actual key if different\n",
    "            how='left',\n",
    "            suffixes=('_high_prob', '_original')\n",
    "        )\n",
    "        \n",
    "        missing_cols = set(cols) - set(merged_df.columns)\n",
    "        if missing_cols:\n",
    "            for col in missing_cols:\n",
    "                merged_df[col] = np.nan  # Assign NaN or appropriate default\n",
    "            logging.warning(f\"Added missing columns with default values: {missing_cols}\")\n",
    "        \n",
    "        # Select required columns\n",
    "        merged_df = merged_df[cols]\n",
    "        \n",
    "        # Extract features\n",
    "        X_data_high_prob = merged_df[cols_t].to_numpy()\n",
    "        \n",
    "        # Predict class probabilities\n",
    "        probabilities = classifier.predict_proba(X_data_high_prob)\n",
    "        \n",
    "        # Define the number of indices you want per class\n",
    "        selected_indices = []\n",
    "        \n",
    "        for class_idx in range(probabilities.shape[1]):\n",
    "            class_prob = probabilities[:, class_idx]\n",
    "            filtered_indices = np.where(class_prob > threshold)[0]\n",
    "            if len(filtered_indices) > num_per_class:\n",
    "                selected = np.random.choice(filtered_indices, size=num_per_class, replace=False)\n",
    "            else:\n",
    "                selected = filtered_indices\n",
    "            selected_indices.extend(selected)\n",
    "        \n",
    "        # Remove duplicates if any\n",
    "        selected_indices = list(set(selected_indices))\n",
    "        logging.info(f\"Selected {len(selected_indices)} high-probability data points.\")\n",
    "        \n",
    "        # Extract the corresponding rows\n",
    "        df_high_prob_selected = merged_df.iloc[selected_indices]\n",
    "        \n",
    "        # Ensure that df_high_prob_selected has all required columns\n",
    "        missing_cols_final = set(cols) - set(df_high_prob_selected.columns)\n",
    "        if missing_cols_final:\n",
    "            raise ValueError(f\"The following required columns are missing in df_high_prob_selected: {missing_cols_final}\")\n",
    "        \n",
    "        return df_high_prob_selected\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error selecting high-probability data points: {e}\")\n",
    "        raise\n",
    "\n",
    "def generate_images(df_new, cols, outdir):\n",
    "    \"\"\"Generate and save images, and prepare manifest.\"\"\"\n",
    "    # Ensure 'TOD' and other necessary columns are present in df_new\n",
    "    required_columns = ['TOD', 'Start Index', 'Stop Index']\n",
    "    missing_required = set(required_columns) - set(df_new.columns)\n",
    "    if missing_required:\n",
    "        raise ValueError(f\"Missing required columns in df_new: {missing_required}\")\n",
    "    \n",
    "    tod_names = df_new['TOD'].unique()\n",
    "    logging.info(f\"Found {len(tod_names)} unique TODs to process.\")\n",
    "    \n",
    "    pbar = tqdm(total=len(tod_names), desc=\"Generating Images\")\n",
    "    df_tosave_all = []\n",
    "    \n",
    "    for tod_name in tod_names:\n",
    "        df_temp = df_new[df_new['TOD'] == tod_name].reset_index(drop=True)\n",
    "        df_temp['Focal Plane Image'] = 'temp'\n",
    "        df_temp['Time Stream Image'] = 'temp'\n",
    "        \n",
    "        tod_file_path = os.path.join('/home/yilun/shared/depots/yilun/', tod_name)\n",
    "        try:\n",
    "            # Check if the TOD file exists\n",
    "            if not os.path.isfile(tod_file_path):\n",
    "                raise FileNotFoundError(f\"TOD file not found: {tod_file_path}\")\n",
    "            \n",
    "            # Load the TOD\n",
    "            tod = cl.load_tod(\n",
    "                tod_name,\n",
    "                depot='/home/yilun/shared/depots/yilun/',\n",
    "                autoloads=['cuts', 'partial', 'cal'],\n",
    "                release='20230220'\n",
    "            )\n",
    "            cl.quick_transform(tod, steps=['demean', 'detrend', 'ff_mce', 'cal', 'f_glitch'])\n",
    "            logging.info(f\"Loaded and transformed TOD: {tod_name}\")\n",
    "            \n",
    "        except FileNotFoundError as fnf_error:\n",
    "            logging.error(fnf_error)\n",
    "            # Skip image generation for the missing TOD and continue\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load or transform TOD {tod_name}: {e}\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        # Get detectors that pass det cuts\n",
    "        dets = tod.cuts.get_uncut()\n",
    "        \n",
    "        # Create a mask for uncut detectors\n",
    "        det_mask = np.zeros(len(tod.det_uid), dtype=bool)\n",
    "        det_mask[dets] = True\n",
    "        \n",
    "        # Collapse all partial cuts to see how many detectors are cut at each time\n",
    "        try:\n",
    "            n_affected = np.sum([tod.pcuts.cuts[det].get_mask() for det in dets], axis=0)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculating n_affected for TOD {tod_name}: {e}\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        # Initialize time slices\n",
    "        tslices = np.zeros((len(df_temp), 2), dtype=int)\n",
    "        \n",
    "        for i in range(len(df_temp)):\n",
    "            try:\n",
    "                tslices[i] = [\n",
    "                    int(df_temp.loc[i, 'Start Index']),\n",
    "                    int(df_temp.loc[i, 'Stop Index'])\n",
    "                ]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error parsing time slices for TOD {tod_name}, row {i}: {e}\")\n",
    "                tslices[i] = [0, 0]  # Assign default values or handle appropriately\n",
    "        \n",
    "        # Generate snippets\n",
    "        try:\n",
    "            snippets = gl.affected_snippets_from_cv(tod, tod.pcuts, tslices, det_mask)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating snippets for TOD {tod_name}: {e}\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        for i, snip in enumerate(snippets):\n",
    "            try:\n",
    "                # Assign image names\n",
    "                focal_image_name = f'TOD_{tod_name}_entire_array_tslice_{tslices[i][0]}-{tslices[i][1]}.png'\n",
    "                timestr_image_name = f'TOD_{tod_name}_snipTOD_tslice_{tslices[i][0]}-{tslices[i][1]}.png'\n",
    "                df_temp.at[i, 'Focal Plane Image'] = focal_image_name\n",
    "                df_temp.at[i, 'Time Stream Image'] = timestr_image_name\n",
    "                \n",
    "                _dets_affected = np.zeros(len(tod.det_uid), dtype=bool)\n",
    "                _dets_affected[snip.det_uid] = True\n",
    "                _dets_affected = _dets_affected.astype(int)\n",
    "                sel_f090 = tod.info.array_data['fcode'] == tod_name[-4:]\n",
    "                \n",
    "                # Plot and save Focal Plane Image\n",
    "                array_plots(\n",
    "                    _dets_affected[sel_f090],\n",
    "                    det=tod.det_uid[sel_f090],\n",
    "                    tod=tod,\n",
    "                    cmap=cm.rainbow,\n",
    "                    display='save',\n",
    "                    save_name=os.path.join(outdir, focal_image_name)\n",
    "                )\n",
    "                logging.info(f\"Saved Focal Plane Image: {focal_image_name}\")\n",
    "                \n",
    "                # Plot and save Time Stream Image\n",
    "                plt.figure()\n",
    "                snip.demean().plot(color='purple', alpha=0.2)\n",
    "                plt.grid(ls='--')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(\n",
    "                    os.path.join(outdir, timestr_image_name),\n",
    "                    dpi=100\n",
    "                )\n",
    "                plt.close()  # Close the figure to free memory\n",
    "                logging.info(f\"Saved Time Stream Image: {timestr_image_name}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error generating images for TOD {tod_name}, snippet {i}: {e}\")\n",
    "                continue  # Skip to the next snippet\n",
    "        \n",
    "        # Append to the list\n",
    "        df_tosave_all.append(df_temp[['Focal Plane Image', 'Time Stream Image'] + cols])\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Concatenate all saved data\n",
    "    if df_tosave_all:\n",
    "        try:\n",
    "            df_tosave = pd.concat(df_tosave_all, ignore_index=True)\n",
    "            logging.info(f\"Total data points to save: {len(df_tosave)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error concatenating df_tosave_all: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        logging.error(\"No data points were processed successfully. df_tosave_all is empty.\")\n",
    "        # Create an empty DataFrame with the required columns\n",
    "        df_tosave = pd.DataFrame(columns=['Focal Plane Image', 'Time Stream Image'] + cols)\n",
    "    \n",
    "    return df_tosave\n",
    "\n",
    "def save_manifest(df_tosave, outdir, manifest_filename='manifest.csv'):\n",
    "    \"\"\"Save the manifest DataFrame to CSV.\"\"\"\n",
    "    try:\n",
    "        manifest_path = os.path.join(outdir, manifest_filename)\n",
    "        df_tosave.to_csv(manifest_path, index=False)\n",
    "        logging.info(f\"Saved manifest to {manifest_path}\")\n",
    "        return manifest_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving manifest: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_active_learning(\n",
    "    df_dir,\n",
    "    df_test_name,\n",
    "    df_train_name,\n",
    "    df_train_high_prob_name,\n",
    "    cols_t,\n",
    "    cols,\n",
    "    outdir,\n",
    "    n_queries=5,\n",
    "    high_prob_threshold=0.6,\n",
    "    high_prob_num_per_class=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to run active learning and generate manifest.csv.\n",
    "    \n",
    "    Parameters:\n",
    "        df_dir (str): Directory containing the CSV files.\n",
    "        df_test_name (str): Filename for the test DataFrame (without .csv).\n",
    "        df_train_name (str): Filename for the train DataFrame (without .csv).\n",
    "        df_train_high_prob_name (str): Filename for the high-prob train DataFrame (without .csv).\n",
    "        cols_t (list): Columns used for training.\n",
    "        cols (list): All columns to be saved in the dataframe at the end.\n",
    "        outdir (str): Output directory for images and manifest.\n",
    "        n_queries (int): Number of active learning queries.\n",
    "        high_prob_threshold (float): Probability threshold for high-probability data points.\n",
    "        high_prob_num_per_class (int): Number of high-probability data points per class.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the generated manifest.csv file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        setup_logging()\n",
    "        df_test, df_train, df_train_high_prob = load_data(df_dir, df_test_name, df_train_name, df_train_high_prob_name)\n",
    "        learner, X_start, Y_start = initialize_active_learner(df_train, cols_t, n_trees=50, max_depth=15)\n",
    "        df_new, X_add, Y_add = perform_active_learning(learner, df_test, cols, cols_t, n_queries=n_queries)\n",
    "        df_high_prob_selected = select_high_probability_data(\n",
    "            df_train_high_prob,\n",
    "            df_train,\n",
    "            learner.estimator,\n",
    "            cols_t,\n",
    "            cols,\n",
    "            threshold=high_prob_threshold,\n",
    "            num_per_class=high_prob_num_per_class\n",
    "        )\n",
    "        df_new = pd.concat([df_new, df_high_prob_selected], ignore_index=True)\n",
    "        logging.info(f\"Added {len(df_high_prob_selected)} high-probability data points to df_new.\")\n",
    "        print(f\"Added {len(df_high_prob_selected)} high-probability data points to df_new.\")\n",
    "        df_tosave = generate_images(df_new, cols, outdir)\n",
    "        manifest_path = save_manifest(df_tosave, outdir)\n",
    "        logging.info(\"Active learning and image generation completed successfully.\")\n",
    "        return manifest_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in run_active_learning: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from active_learning_module import run_active_learning\n",
    "from panoptes_client import Panoptes, Workflow, SubjectSet\n",
    "from panoptes_client import Panoptes\n",
    "from panoptes_client.panoptes import Panoptes\n",
    "from panoptes_client import SubjectSet, Subject\n",
    "import logging\n",
    "\n",
    "def create_subject_set(name,workflow_id):\n",
    "    \"\"\"Create a new Subject Set.\"\"\"\n",
    "    try:\n",
    "        subject_set = SubjectSet()\n",
    "        subject_set.links.workflow = Workflow.find(workflow_id).id\n",
    "        subject_set.display_name = name\n",
    "        subject_set.save()\n",
    "        logging.info(f\"Created new Subject Set: {name}\")\n",
    "        return subject_set\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating Subject Set: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_bash_command(command, shell='/bin/bash'):\n",
    "    print(f\"Running command: {command}\")\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, executable=shell, text=True)\n",
    "\n",
    "    # Read the output in real-time\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "    \n",
    "    # Capture any remaining error output\n",
    "    stderr = process.stderr.read()\n",
    "    if stderr:\n",
    "        print(f\"Command error: {stderr.strip()}\")\n",
    "    \n",
    "    return process.returncode\n",
    "\n",
    "def upload_subjects(subject_set_id, manifest_path):\n",
    "    \"\"\"Upload subjects to the specified Subject Set from the manifest file.\"\"\"\n",
    "    command = f\"panoptes subject-set upload-subjects {subject_set_id} {manifest_path}\"\n",
    "    return run_bash_command(command)\n",
    "\n",
    "def main():\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        filename='framework.log',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s:%(levelname)s:%(message)s'\n",
    "    )\n",
    "    \n",
    "    # Securely fetch credentials from environment variables\n",
    "    username = os.getenv('ZOONIVERSE_USERNAME')\n",
    "    password = os.getenv('ZOONIVERSE_PASSWORD')\n",
    "    \n",
    "    if not username or not password:\n",
    "        logging.error(\"Zooniverse credentials not set in environment variables.\")\n",
    "        print(\"Error: Zooniverse credentials not set in environment variables.\")\n",
    "        return\n",
    "    \n",
    "    description = 'Zooniverse Python client-backed project'\n",
    "    \n",
    "    # Connect to Panoptes\n",
    "    try:\n",
    "        Panoptes.connect(username=username, password=password)\n",
    "        logging.info(\"Connected to Panoptes successfully.\")\n",
    "        print(\"Connection established.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to connect to Panoptes: {e}\")\n",
    "        print(\"Failed to connect to Panoptes.\")\n",
    "        return\n",
    "    \n",
    "    # Specify your workflow ID here\n",
    "\n",
    "    workflow_id = os.getenv('WORKFLOW_ID')\n",
    "    try:\n",
    "        workflow = Workflow.find(workflow_id)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to find Workflow with ID {workflow_id}: {e}\")\n",
    "        print(f\"Failed to find Workflow with ID {workflow_id}.\")\n",
    "        return\n",
    "    \n",
    "    if workflow.subjects_count == workflow.retired_set_member_subjects_count: \n",
    "        # When it's the initial run or if all data from the previous batch has been labeled \n",
    "        # then trigger active learning loop\n",
    "        subject_set_name = input(\"Enter subject set name: \")\n",
    "        try:\n",
    "            subject_set = create_subject_set(subject_set_name, workflow_id)\n",
    "            subject_set_id = subject_set.id \n",
    "            print(f\"New subject set created, with Subject Set ID: {subject_set_id}\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create Subject Set.\")\n",
    "            return\n",
    "    \n",
    "        # Define parameters for active learning\n",
    "        df_dir = '/home/simran/examples_for_zoey/' # replace with your data set directory\n",
    "        df_test_name = 'predicted_labels_depth1_1619094281_pa5_f090_wseason' # replace with your test set file name\n",
    "        cols_for_training = ['Number of Detectors', 'Y and X Extent Ratio','Y Hist Max and Adjacent/Number of Detectors',\n",
    "          'Within 0.1 of Y Hist Max/Number of Detectors', 'Mean abs(Correlation)', 'Mean abs(Time Lag)', 'Number of Peaks']\n",
    "          \n",
    "        outdir = '/home/zoey/example_dir' # replace with your output director\n",
    "        n_queries = 5\n",
    "        high_prob_threshold = 0.6\n",
    "        high_prob_num_per_class = 5\n",
    "        \n",
    "        # Run active learning module and output the manifest file for selected data\n",
    "        try:\n",
    "            manifest_path = run_active_learning(\n",
    "                df_dir=df_dir,\n",
    "                df_test_name=df_test_name,\n",
    "                # df_train_name=df_train_name,\n",
    "                # df_train_high_prob_name=df_train_high_prob_name,\n",
    "                cols_t=cols_for_training,\n",
    "                # cols=cols,\n",
    "                outdir=outdir,\n",
    "                n_queries=n_queries,\n",
    "                high_prob_threshold=high_prob_threshold,\n",
    "                high_prob_num_per_class=high_prob_num_per_class\n",
    "            )\n",
    "            print(f\"Active learning completed. Manifest saved at {manifest_path}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Active learning failed: {e}\")\n",
    "            print(\"Active learning failed. Check logs for details.\")\n",
    "            return\n",
    "        \n",
    "        # Get user input for manifest file path\n",
    "        file_path = manifest_path  # Using the output from the module\n",
    "        \n",
    "        # Upload subjects\n",
    "        return_code = upload_subjects(subject_set_id, file_path)\n",
    "        \n",
    "        # Check return code\n",
    "        if return_code == 0:\n",
    "            print(\"Subjects uploaded successfully.\")\n",
    "            logging.info(\"Subjects uploaded successfully.\")\n",
    "        else:\n",
    "            print(\"Subject upload failed.\")\n",
    "            logging.error(\"Subject upload failed.\")\n",
    "    else:\n",
    "        # When the previous batch's labeling is not complete\n",
    "        print(\"Workflow already has active subjects. No action taken.\")\n",
    "        logging.info(\"Workflow already has active subjects. No action taken.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.20 ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efc0e27f76a93e37a557994470219451c5ce5d40d27c8a361881805590154496"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
